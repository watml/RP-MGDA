{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNzzkn8w12Wu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import tqdm\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Refined Partition procedure for general dependency matrices"
      ],
      "metadata": {
        "id": "-8kk8tbf4AYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "G5h09EoF79Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjacency_matrix_to_bipartite_graph(adj_matrix):\n",
        "    # Initialize bipartite graph\n",
        "    B = nx.Graph()\n",
        "\n",
        "    # Number of functions and variables (square matrix assumption)\n",
        "    num_functions = adj_matrix.shape[0]\n",
        "    num_variables = adj_matrix.shape[1]\n",
        "\n",
        "    # Add function nodes\n",
        "    function_nodes = [f'f_{i+1}' for i in range(num_functions)]\n",
        "    B.add_nodes_from(function_nodes, bipartite=0)  # bipartite=0 means these are one set of nodes\n",
        "\n",
        "    # Add variable nodes\n",
        "    variable_nodes = [f'w_{i+1}' for i in range(num_variables)]\n",
        "    B.add_nodes_from(variable_nodes, bipartite=1)  # bipartite=1 means these are the second set of nodes\n",
        "\n",
        "    # Add edges based on the adjacency matrix\n",
        "    for i in range(num_functions):\n",
        "        for j in range(num_variables):\n",
        "            if adj_matrix[i, j] == 1:\n",
        "                B.add_edge(f'f_{i+1}', f'w_{j+1}')\n",
        "\n",
        "    return B, function_nodes, variable_nodes"
      ],
      "metadata": {
        "id": "mnj5Sptb6QLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_bipartite_graph(B, function_nodes, variable_nodes):\n",
        "    # Set the positions for the two sets of nodes: functions on top, variables on the bottom\n",
        "    pos = {}\n",
        "    pos.update((node, (i, 1)) for i, node in enumerate(function_nodes))  # Function nodes on y=1\n",
        "    pos.update((node, (i, -1)) for i, node in enumerate(variable_nodes))  # Variable nodes on y=-1\n",
        "\n",
        "    # Plotting the bipartite graph\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    nx.draw(B, pos, with_labels=True, node_color=['lightblue' if node.startswith('f') else 'lightgreen' for node in B.nodes()],\n",
        "            node_size=2000, font_size=12, font_weight='bold')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ThZcC2tC7yX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_cycle(B):\n",
        "    \"\"\"\n",
        "    Detect a cycle in the graph B using NetworkX's cycle detection method.\n",
        "\n",
        "    Parameters:\n",
        "    B (networkx.Graph): The input bipartite graph.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of edges forming the cycle if found, otherwise an empty list.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cycle = nx.find_cycle(B)\n",
        "        return cycle\n",
        "    except nx.exception.NetworkXNoCycle:\n",
        "        print(\"No cycle found.\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "ox2CKh1l-rx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_highlight_cycle(B, cycle, function_nodes, variable_nodes):\n",
        "    \"\"\"\n",
        "    Plot the bipartite graph B and highlight the edges in the cycle.\n",
        "    Arrange function nodes at the top and variable nodes at the bottom.\n",
        "\n",
        "    Parameters:\n",
        "    B (networkx.Graph): The input bipartite graph.\n",
        "    cycle (list): A list of edges forming the cycle to be highlighted.\n",
        "    function_nodes (list): List of function nodes (e.g., ['f_1', 'f_2', ...]).\n",
        "    variable_nodes (list): List of variable nodes (e.g., ['w_1', 'w_2', ...]).\n",
        "    \"\"\"\n",
        "    # Set the positions for the two sets of nodes: functions on top, variables on the bottom\n",
        "    pos = {}\n",
        "    pos.update((node, (i, 1)) for i, node in enumerate(function_nodes))  # Function nodes on y=1\n",
        "    pos.update((node, (i, -1)) for i, node in enumerate(variable_nodes))  # Variable nodes on y=-1\n",
        "\n",
        "    # Edge color red for edges in the cycle, black otherwise\n",
        "    edge_colors = ['red' if edge in cycle or (edge[1], edge[0]) in cycle else 'black' for edge in B.edges()]\n",
        "\n",
        "    # Plot the graph\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    nx.draw(B, pos, with_labels=True, node_color='lightblue', edge_color=edge_colors, node_size=2000, font_size=12)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZqUwfGM5-ywj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_cycle_variable_nodes(B, cycle, variable_nodes):\n",
        "    \"\"\"\n",
        "    Merge variable nodes within the detected cycle into a single node in a new graph.\n",
        "\n",
        "    Parameters:\n",
        "    B (networkx.Graph): The original bipartite graph.\n",
        "    cycle (list): A list of edges forming the cycle to be merged.\n",
        "    variable_nodes (list): List of variable nodes to be considered for merging.\n",
        "\n",
        "    Returns:\n",
        "    new_graph (networkx.Graph): A new graph with the cycle variables merged.\n",
        "    merged_node_name (str): The name of the new merged node.\n",
        "    merged_variables (list): List of variable nodes that were merged.\n",
        "    \"\"\"\n",
        "    new_graph = B.copy()  # Create a copy of the original graph to modify\n",
        "\n",
        "    # Extract the variable nodes involved in the cycle\n",
        "    cycle_variables = set()\n",
        "    for u, v in cycle:\n",
        "        if u in variable_nodes:\n",
        "            cycle_variables.add(u)\n",
        "        if v in variable_nodes:\n",
        "            cycle_variables.add(v)\n",
        "\n",
        "    # If no variable nodes were found in the cycle, return the original graph\n",
        "    if not cycle_variables:\n",
        "        print(\"No variable nodes in the cycle to merge.\")\n",
        "        return new_graph, None, []\n",
        "\n",
        "    # Create a new node name to represent the merged variables\n",
        "    merged_node_name = \"_\".join(sorted(cycle_variables))  # E.g., \"w_1_w_2\"\n",
        "\n",
        "    # Add the merged node to the new graph\n",
        "    new_graph.add_node(merged_node_name)\n",
        "\n",
        "    # Keep track of the merged variable names\n",
        "    merged_variables = list(cycle_variables)\n",
        "\n",
        "    # Find the functions connected to the cycle variables and link them to the merged node\n",
        "    for var in merged_variables:\n",
        "        # Find neighbors (functions) of the variable node\n",
        "        neighbors = list(B.neighbors(var))\n",
        "        for neighbor in neighbors:\n",
        "            # Add an edge between the merged node and the function\n",
        "            new_graph.add_edge(merged_node_name, neighbor)\n",
        "\n",
        "        # Remove the original variable node from the graph\n",
        "        new_graph.remove_node(var)\n",
        "\n",
        "    return new_graph, merged_node_name, merged_variables"
      ],
      "metadata": {
        "id": "hnQ1k1fODHGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_cycle_detection_and_merge(B, function_nodes, variable_nodes):\n",
        "    \"\"\"\n",
        "    Repeat the process of detecting and merging cycles in the graph B until no cycle is detected.\n",
        "\n",
        "    Parameters:\n",
        "    B (networkx.Graph): The initial bipartite graph.\n",
        "    function_nodes (list): List of function nodes (e.g., ['f_1', 'f_2', ...]).\n",
        "    variable_nodes (list): List of variable nodes (e.g., ['w_1', 'w_2', ...]).\n",
        "\n",
        "    Returns:\n",
        "    new_graph (networkx.Graph): The final graph after all cycles are merged.\n",
        "    merged_variable_list (list): A list of all merged variable nodes.\n",
        "    function_nodes (list): The final list of function nodes.\n",
        "    variable_nodes (list): The final list of variable nodes, including merged nodes.\n",
        "    \"\"\"\n",
        "    merged_variable_list = []\n",
        "    merged_count = 0\n",
        "\n",
        "    # Iteratively detect and merge cycles\n",
        "    while True:\n",
        "        # Detect a cycle in the current graph\n",
        "        cycle = detect_cycle(B)\n",
        "\n",
        "        # If no cycle is detected, break the loop\n",
        "        if not cycle:\n",
        "            break\n",
        "\n",
        "        # Merge variable nodes involved in the detected cycle\n",
        "        B, merged_node, merged_vars = merge_cycle_variable_nodes(B, cycle, variable_nodes)\n",
        "\n",
        "        # If a merge happened, update the variable_nodes list\n",
        "        if merged_node:\n",
        "            # Shorten the name of the merged node to avoid long names\n",
        "            merged_count += 1\n",
        "            shortened_merged_node = f\"merged_{merged_count}\"\n",
        "\n",
        "            # Rename the merged node in the graph\n",
        "            B = nx.relabel_nodes(B, {merged_node: shortened_merged_node})\n",
        "\n",
        "            # Update the list of variable nodes by removing merged ones and adding the new one\n",
        "            variable_nodes = [node for node in variable_nodes if node not in merged_vars]\n",
        "            variable_nodes.append(shortened_merged_node)\n",
        "\n",
        "            # Keep track of merged variables for later reference\n",
        "            merged_variable_list.append((shortened_merged_node, merged_vars))\n",
        "\n",
        "    # Return the final graph, merged variable list, and updated function/variable nodes\n",
        "    return B, merged_variable_list, function_nodes, variable_nodes\n"
      ],
      "metadata": {
        "id": "kBQFJEkSDZVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process_merged_variables(merged_variable_list):\n",
        "    \"\"\"\n",
        "    Post-process merged variable list to flatten any nested merged variables.\n",
        "\n",
        "    Parameters:\n",
        "    merged_variable_list (list): List of tuples where each tuple contains a merged node and its merged variables.\n",
        "\n",
        "    Returns:\n",
        "    list: The post-processed list with flattened merged variables.\n",
        "    \"\"\"\n",
        "    # Create a mapping from each merged node to the variables it includes\n",
        "    merged_dict = {merged_node: set(vars) for merged_node, vars in merged_variable_list}\n",
        "\n",
        "    # Helper function to recursively gather all original variables\n",
        "    def get_original_vars(node):\n",
        "        if node in merged_dict:\n",
        "            # Recursively flatten merged variables\n",
        "            result = set()\n",
        "            for var in merged_dict[node]:\n",
        "                result.update(get_original_vars(var))\n",
        "            return result\n",
        "        else:\n",
        "            # This is an original variable (w_1, w_2, etc.)\n",
        "            return {node}\n",
        "\n",
        "    # Flatten all merged variables\n",
        "    flattened_list = []\n",
        "    for merged_node, vars in merged_variable_list:\n",
        "        original_vars = get_original_vars(merged_node)\n",
        "        flattened_list.append((merged_node, list(original_vars)))\n",
        "\n",
        "    return flattened_list"
      ],
      "metadata": {
        "id": "CHjEMnEbDe5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_existing_merged_nodes(flattened_merged_variable_list, final_graph):\n",
        "    \"\"\"\n",
        "    Filter and print only the merged variables that still exist in the final graph.\n",
        "\n",
        "    Parameters:\n",
        "    flattened_merged_variable_list (list): List of flattened merged nodes and their original variables.\n",
        "    final_graph (networkx.Graph): The final graph after all merging steps.\n",
        "\n",
        "    Returns:\n",
        "    list: A filtered list of merged nodes that still exist in the final graph.\n",
        "    \"\"\"\n",
        "    # Get the nodes that exist in the final graph\n",
        "    existing_nodes = set(final_graph.nodes())\n",
        "\n",
        "    # Filter the merged variable list to include only the nodes that still exist in the graph\n",
        "    filtered_list = [(merged_node, vars) for merged_node, vars in flattened_merged_variable_list if merged_node in existing_nodes]\n",
        "\n",
        "    return filtered_list"
      ],
      "metadata": {
        "id": "Ip_7PwHaDln1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sparse_adj_matrix(rows, cols, density, seed=None):\n",
        "    \"\"\"\n",
        "    Generates a random sparse adjacency matrix with at least one non-zero entry in each column\n",
        "    and ensures that the num_non_zero 1s are assigned exactly to empty entries.\n",
        "\n",
        "    Parameters:\n",
        "    rows (int): Number of rows.\n",
        "    cols (int): Number of columns.\n",
        "    density (float): Approximate density of non-zero elements (between 0 and 1).\n",
        "    seed (int, optional): Random seed for reproducibility. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: The generated sparse adjacency matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    adj_matrix = np.zeros((rows, cols), dtype=int)\n",
        "\n",
        "    # Ensure at least one non-zero entry in each column\n",
        "    for j in range(cols):\n",
        "        adj_matrix[random.randint(0, rows - 1), j] = 1\n",
        "\n",
        "    # Calculate the number of additional non-zero entries needed\n",
        "    num_elements = rows * cols\n",
        "    num_non_zero = int(density * num_elements)\n",
        "    additional_non_zero = num_non_zero - cols  # Subtract cols as we already have one per column\n",
        "\n",
        "    # Find indices of empty entries\n",
        "    empty_indices = np.where(adj_matrix == 0)\n",
        "    num_empty_entries = len(empty_indices[0])\n",
        "\n",
        "    # If there are enough empty entries, assign 1s to them\n",
        "    if additional_non_zero <= num_empty_entries:\n",
        "        # Randomly select indices to assign 1s\n",
        "        selected_indices = random.sample(range(num_empty_entries), additional_non_zero)\n",
        "\n",
        "        # Assign 1s to the selected empty entries\n",
        "        for index in selected_indices:\n",
        "            row = empty_indices[0][index]\n",
        "            col = empty_indices[1][index]\n",
        "            adj_matrix[row, col] = 1\n",
        "    else:\n",
        "        # If there are not enough empty entries, fill all remaining empty entries with 1s\n",
        "        adj_matrix[empty_indices] = 1\n",
        "\n",
        "    return adj_matrix"
      ],
      "metadata": {
        "id": "iB54Ww4eDqwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The impact of adjacency matrix density on variable merging"
      ],
      "metadata": {
        "id": "BUS0vuDC5s6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment parameters\n",
        "rows = 100\n",
        "cols = 100\n",
        "densities = [0.015, 0.018, 0.02, 0.025, 0.03, 0.04, 0.05, 0.06, 0.07, 0.1]\n",
        "num_trials = 100\n",
        "\n",
        "# Store results\n",
        "results = {density: [] for density in densities}\n",
        "\n",
        "# Run the experiment\n",
        "for density in densities:\n",
        "    for trial in range(num_trials):\n",
        "        # Generate sparse adjacency matrix with a random seed\n",
        "        seed = trial  # Use trial number as seed for different random matrices\n",
        "        adj_matrix = generate_sparse_adj_matrix(rows, cols, density, seed=seed)\n",
        "\n",
        "        # Create bipartite graph\n",
        "        B, function_nodes, variable_nodes = adjacency_matrix_to_bipartite_graph(adj_matrix)\n",
        "\n",
        "        # Run iterative cycle detection and merging\n",
        "        final_graph, merged_variable_list, function_nodes, variable_nodes = iterative_cycle_detection_and_merge(B, function_nodes, variable_nodes)\n",
        "\n",
        "        # Record the number of variable nodes in the final graph\n",
        "        results[density].append(len(variable_nodes))  # Store results as len(variable_nodes)\n",
        "\n",
        "# Print results\n",
        "for density, values in results.items():\n",
        "    # Calculate and print the average number of variable nodes\n",
        "    avg_variable_nodes = np.mean(values)  # Directly calculate the mean of the values list\n",
        "    print(f\"Density: {density}, Average variable nodes: {avg_variable_nodes}\")"
      ],
      "metadata": {
        "id": "jMda6u0SyFRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Store results in a pickle file\n",
        "filename = 'sparsity_results_dict.pickle'\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(results, file)"
      ],
      "metadata": {
        "id": "Z0n3e1FuB-s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "VxmwYkYd7t3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sparsity_results_dict.pickle', 'rb') as file:\n",
        "    results = pickle.load(file)"
      ],
      "metadata": {
        "id": "auCamXoNPuSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the box plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Prepare data for box plot\n",
        "data = [np.sqrt(values) for values in results.values()]  # Apply square root transformation\n",
        "labels = ['{:.1f}'.format(density * 100) for density in results.keys()]\n",
        "\n",
        "# Exclude the last density\n",
        "data = data[:-1]  # Slice data to exclude the last element\n",
        "labels = labels[:-1]  # Slice labels to exclude the last element\n",
        "\n",
        "medianprops = dict(color='orange', linewidth=2)\n",
        "whiskerprops = dict(color='black')\n",
        "capprops = dict(color='black', linewidth=1.5)\n",
        "plt.boxplot(data, labels=labels, showfliers=False, showmeans=False, medianprops = medianprops,\n",
        "            whiskerprops=whiskerprops, capprops=capprops)  # Create box plot\n",
        "\n",
        "# Increase tick label font size\n",
        "plt.tick_params(axis='both', which='major', labelsize=12)\n",
        "plt.xlabel(\"Density (%)\",fontsize=17)\n",
        "plt.ylabel(\"# of Variable Blocks (Square root)\",fontsize=17)  # Update y-axis label\n",
        "plt.title(\"Effect of Sparsity on Variable Merging\",fontsize=20)\n",
        "plt.grid(True)\n",
        "# Add y-tick at y=1 and remove y-tick at y=12\n",
        "yticks = list(plt.yticks()[0])  # Get current y-ticks\n",
        "yticks = [tick for tick in yticks if tick != 12]  # Remove y-tick at y=12\n",
        "yticks.append(1)  # Add y-tick at y=1\n",
        "plt.yticks(yticks)\n",
        "\n",
        "plt.savefig('boxplot_sparsity_bigger_font.png', dpi=300)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "hpjJfvRj_crs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MGDA vs RP-MGDA on quadratic MOO problem"
      ],
      "metadata": {
        "id": "kyhh-pqXFrT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MGDA helper functions"
      ],
      "metadata": {
        "id": "usvPeoMY3KtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quadprog"
      ],
      "metadata": {
        "id": "lbvbSMdy2qdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import quadprog"
      ],
      "metadata": {
        "id": "lLkC2LOq2xv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QP package to solve for the min norm element\n",
        "def solve_w(U):\n",
        "    n,d = U.shape\n",
        "    K = np.eye(n,dtype=float)\n",
        "    for i in range(0,n):\n",
        "        for j in range(0,n):\n",
        "            K[i,j]= np.dot(U[i],U[j])\n",
        "    Q = 0.5 *(K + K.T)\n",
        "    p = np.zeros(n,dtype=float)\n",
        "    a = np.ones(n,dtype=float).reshape(-1,1)\n",
        "    Id = np.eye(n,dtype=float)\n",
        "    A = np.concatenate((a,Id),axis=1)\n",
        "    b = np.zeros(n+1)\n",
        "    b[0] = 1.\n",
        "    grad = np.zeros(d,dtype=float)\n",
        "    # grad = np.zeros(n,dtype=float)\n",
        "    try:\n",
        "        grad = U.T.dot(quadprog.solve_qp(Q,p,A,b)[0])\n",
        "    except ValueError as v:\n",
        "        print('MGDA stops since the min norm element is zero')\n",
        "    return grad"
      ],
      "metadata": {
        "id": "yuYwLE5C25uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a FW procedure to solve for the min norm element\n",
        "# step_size (sz=2/(t+2))\n",
        "def FW_solve_w(U, rounds):\n",
        "  n,d = U.shape\n",
        "  lbd = 1.0/n * np.ones(n)\n",
        "  G = np.array(U.dot(U.T))\n",
        "  for t in range(rounds):\n",
        "    v = G.dot(lbd)\n",
        "    idx_min = np.argmin(v)\n",
        "    d = np.zeros(n,dtype=float)\n",
        "    d[idx_min] = 1\n",
        "\n",
        "    sz = 2.0/(t+2)\n",
        "    lbd = (1-sz) * lbd + sz * d\n",
        "\n",
        "  return U.T.dot(lbd), lbd"
      ],
      "metadata": {
        "id": "U7iUR3y93EVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sz(a,b):\n",
        "  # find the minimizer of ax^2 + 2bx, where 0<=x<=1 (x is step size)\n",
        "\n",
        "  # check that a is non negative\n",
        "  if a < 0 :\n",
        "    print(\"error in solving step size\")\n",
        "    return -1\n",
        "  if a == 0:\n",
        "    if b >=0 :\n",
        "      sz = 0\n",
        "    else:\n",
        "      sz = 1\n",
        "    return sz\n",
        "\n",
        "  axis = - b * 1.0 / a\n",
        "  if axis < 0 :\n",
        "    sz = 0\n",
        "  elif axis > 1:\n",
        "    sz = 1\n",
        "  else:\n",
        "    sz = axis\n",
        "  return sz"
      ],
      "metadata": {
        "id": "kiN7lVX33EvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a FW procedure to solve for the min norm element\n",
        "# with exact step size\n",
        "def new_FW_solve_w(U, rounds, lambda_0=None, to_print=False):\n",
        "  # with warm start of lbd\n",
        "  n,d = U.shape\n",
        "  if lambda_0 is None:\n",
        "    lbd = 1.0/n * np.ones(n)\n",
        "  else:\n",
        "    lbd = lambda_0\n",
        "\n",
        "  # precomputing G here is actually better since it can be reused in the loop,\n",
        "  # otherwise matrix/matrix multiplication (UU^T) is not efficient\n",
        "  G = np.array(U.dot(U.T))\n",
        "\n",
        "  for t in range(rounds):\n",
        "    v = G.dot(lbd)\n",
        "    idx_min = np.argmin(v)\n",
        "    d = np.zeros(n,dtype=float)\n",
        "    d[idx_min] = 1\n",
        "\n",
        "    # find the best sz by solving a quadratic problem\n",
        "    delta = d - lbd\n",
        "    a = delta.dot(G.dot(delta))\n",
        "    b = delta.dot(v)\n",
        "    sz = find_sz(a,b)\n",
        "    if sz < 0:\n",
        "      sys.exit(\"error in running FW for solving QP\")\n",
        "\n",
        "    lbd = (1-sz) * lbd + sz * d\n",
        "    if sz == 0 and to_print:\n",
        "      print(\"it takes ith round:\",t)\n",
        "      break\n",
        "\n",
        "  return U.T.dot(lbd), lbd"
      ],
      "metadata": {
        "id": "Kp7JdIvr3G5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RP procedure"
      ],
      "metadata": {
        "id": "i5s7XiIS_iSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(w, D_matrix):\n",
        "    \"\"\"\n",
        "    Compute the function values and gradients for each f_i, where each f_i depends on\n",
        "    the variables indicated in the input adjacency matrix D_matrix, and each f_i is a\n",
        "    quadratic function of the variables it depends on.\n",
        "\n",
        "    Parameters:\n",
        "    w (np.ndarray): Input vector of variables (size d).\n",
        "    D_matrix (np.ndarray): Dependency matrix (n x d), where D_matrix[i, j] = 1\n",
        "                           indicates that function f_i depends on variable w_j.\n",
        "\n",
        "    Returns:\n",
        "    tuple: (f_val, grad_f)\n",
        "           f_val: List of function values for each f_i (length n).\n",
        "           grad_f: List of gradient vectors for each f_i (each vector is of length d).\n",
        "    \"\"\"\n",
        "    d = w.size  # Number of variables (columns of D_matrix)\n",
        "    n = D_matrix.shape[0]  # Number of functions (rows of D_matrix)\n",
        "\n",
        "    f_val = [0] * n  # List to store function values\n",
        "    grad_f = [0] * n  # List to store gradient vectors\n",
        "\n",
        "    for i in range(n):\n",
        "        D_row = D_matrix[i]  # Get the dependency row for f_i\n",
        "        f_val[i] = 0\n",
        "        grad_f[i] = np.zeros(d)\n",
        "\n",
        "        for j in range(d):\n",
        "            if D_row[j] != 0:  # If f_i depends on variable w_j\n",
        "                # Compute the quadratic function value and gradient for w_j\n",
        "                f_val[i] += (w[j] - i) ** 2\n",
        "                grad_f[i][j] = 2 * (w[j] - i)\n",
        "\n",
        "    return f_val, grad_f"
      ],
      "metadata": {
        "id": "oLAyClB9_cuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a random 20x100 dependency matrix\n",
        "D_matrix_20x100 = generate_sparse_adj_matrix(20, 100, 0.1, seed=3)\n",
        "\n",
        "# Create the bipartite graph\n",
        "B, function_nodes, variable_nodes = adjacency_matrix_to_bipartite_graph(D_matrix_20x100)\n",
        "\n",
        "# Run the iterative cycle detection and merging process\n",
        "final_graph, merged_variable_list, function_nodes, variable_nodes = iterative_cycle_detection_and_merge(B, function_nodes, variable_nodes)\n",
        "\n",
        "\n",
        "# Post-process the merged variable list to flatten any nested merges\n",
        "flattened_merged_variable_list = post_process_merged_variables(merged_variable_list)\n",
        "\n",
        "# Filter the merged variable list to only print those that exist in the final graph\n",
        "existing_merged_variables = filter_existing_merged_nodes(flattened_merged_variable_list, final_graph)\n",
        "\n",
        "# Create a dictionary mapping merged variables to their constituent variables\n",
        "# Using only existing_merged_variables\n",
        "merged_var_dict = {}\n",
        "for merged_var_group in existing_merged_variables:\n",
        "    merged_var = merged_var_group[0]\n",
        "    constituent_vars = merged_var_group[1:]\n",
        "    merged_var_dict[merged_var] = constituent_vars\n",
        "\n",
        "# Update variable_nodes by replacing merged variables with their constituents\n",
        "updated_variable_nodes = []\n",
        "for var in variable_nodes:\n",
        "    if var in merged_var_dict:\n",
        "        updated_variable_nodes.extend(merged_var_dict[var])  # Replace with constituents\n",
        "    else:\n",
        "        updated_variable_nodes.append(var)  # Keep original variable\n",
        "\n",
        "# Output the results\n",
        "print(\"Final list of existing merged variable nodes:\")\n",
        "for merged_node, vars in existing_merged_variables:\n",
        "    print(f\"{merged_node}: merged from {vars}\")\n",
        "\n",
        "num_variable_nodes_final = len(variable_nodes)\n",
        "print(f\"Number of variable nodes in final graph: {num_variable_nodes_final}\")\n"
      ],
      "metadata": {
        "id": "hbY160KF_cwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MDGA"
      ],
      "metadata": {
        "id": "uYmDAd-GAGyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial values for w_0 (size 100)\n",
        "scaling_factor = 1  # Adjust this value to control the scaling\n",
        "seed = 100\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "w_0 = np.random.rand(100) * scaling_factor\n",
        "w = w_0\n",
        "norm_list = []\n",
        "\n",
        "# Start measuring time\n",
        "start_time = time.time()\n",
        "\n",
        "# Main MGDA loop for the 20x100 matrix\n",
        "for i in range(1000): # or 800\n",
        "    # Compute function values and gradients for the current w and D_matrix\n",
        "    f_val, grad_f = f(w, D_matrix_20x100)\n",
        "\n",
        "    # Stack gradients into a matrix U\n",
        "    U = np.vstack(grad_f)\n",
        "\n",
        "    # Solve for the gradient direction using Frank-Wolfe\n",
        "    grad, _ = new_FW_solve_w(U,rounds=10)\n",
        "\n",
        "    # Record the norm of the gradient\n",
        "    norm_list.append(np.linalg.norm(grad))\n",
        "\n",
        "    # Update w using the step size sz\n",
        "    sz = 0.01\n",
        "    w = w - sz * grad\n",
        "\n",
        "    # Print the logarithm of the norm of the gradient after 400 iterations\n",
        "    if i == 799:\n",
        "        print(np.linalg.norm(grad))\n",
        "        print(w)  # Print the final value of w\n",
        "        print(f(w, D_matrix_20x100)[0])  # Print the function values f(w)\n",
        "\n",
        "# End measuring time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the runtime\n",
        "runtime = end_time - start_time\n",
        "print(f\"Runtime: {runtime} seconds\")\n"
      ],
      "metadata": {
        "id": "0XJEd1ibUSDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_w = w.copy()\n",
        "store_func_values = f(w, D_matrix_20x100)[0].copy()"
      ],
      "metadata": {
        "id": "Y7p512w2UcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RPMGDA"
      ],
      "metadata": {
        "id": "vSfEVOOoZqKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variable_nodes = updated_variable_nodes.copy()\n",
        "D_matrix = D_matrix_20x100.copy()"
      ],
      "metadata": {
        "id": "M3Lj-hpPhYft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert string variable names ('w_i') to numerical indices\n",
        "def var_name_to_index(var_name):\n",
        "    \"\"\"\n",
        "    Convert a variable name of the form 'w_i' to an index (i-1), assuming 'i' starts from 1.\n",
        "    \"\"\"\n",
        "    return int(var_name.split('_')[1]) - 1  # Convert 'w_i' to index i-1"
      ],
      "metadata": {
        "id": "yK8fimvW_x0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute the dependencies for each block\n",
        "def precompute_dependencies(D_matrix, variable_nodes):\n",
        "    \"\"\"\n",
        "    Precompute and store the dependent functions for each variable/block in variable_nodes.\n",
        "\n",
        "    Returns:\n",
        "    dependencies: List of tuples (block, dependent_functions, use_mgda)\n",
        "                  where block is a list of variable indices, dependent_functions are the functions that depend on the block,\n",
        "                  and use_mgda is a boolean indicating whether MGDA should be used.\n",
        "    \"\"\"\n",
        "    dependencies = []\n",
        "\n",
        "    for block in variable_nodes:\n",
        "        if isinstance(block, list):\n",
        "            # Convert block list from strings to indices\n",
        "            block_indices = [var_name_to_index(var) for var in block]\n",
        "\n",
        "            # Identify which functions depend on this block\n",
        "            dependent_functions = []\n",
        "            for j in range(D_matrix.shape[0]):  # Loop over functions\n",
        "                if np.any(D_matrix[j, block_indices]):  # If any variable in block is dependent\n",
        "                    dependent_functions.append(j)\n",
        "\n",
        "            use_mgda = len(dependent_functions) >= 2\n",
        "            dependencies.append((block_indices, dependent_functions, use_mgda))\n",
        "        else:\n",
        "            # Single variable, convert from string to index\n",
        "            block_index = var_name_to_index(block)\n",
        "\n",
        "            # Identify which functions depend on this single variable\n",
        "            dependent_func_idx = np.where(D_matrix[:, block_index] == 1)[0]\n",
        "\n",
        "            use_mgda = len(dependent_func_idx) >= 2\n",
        "            dependencies.append(([block_index], dependent_func_idx, use_mgda))\n",
        "\n",
        "    return dependencies"
      ],
      "metadata": {
        "id": "mUNheEd4hd8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute the dependencies outside of the loop\n",
        "dependencies = precompute_dependencies(D_matrix, variable_nodes)\n",
        "\n",
        "# RP-MGDA with same random initialization\n",
        "w = w_0.copy()\n",
        "\n",
        "# Use the following initialization if testing refinement of MGDA solution with RP-MGDA\n",
        "# w = store_w.copy()\n",
        "\n",
        "norm_list_RPMGDA = []\n",
        "\n",
        "sz = 0.01  # Step size for gradient descent\n",
        "\n",
        "# Start measuring time\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(1000):  # Running for 1000 iterations; postprocess with 5 iterations\n",
        "    f_val, grad_f = f(w, D_matrix)  # Compute function values and gradients\n",
        "    Jcb_f = np.stack(grad_f, axis=0)  # Jacobian of gradients\n",
        "    norm_sq = 0\n",
        "\n",
        "    # Use the precomputed dependencies to update blocks\n",
        "    for block_indices, dependent_functions, use_mgda in dependencies:\n",
        "        if use_mgda:\n",
        "            # Use MGDA for blocks dependent on 2 or more functions\n",
        "            U = Jcb_f[dependent_functions][:, block_indices]  # Extract rows corresponding to dependent functions\n",
        "            grad, _ = new_FW_solve_w(U, rounds=10)  # Solve with MGDA step\n",
        "            w[block_indices] = w[block_indices] - sz * grad  # Update block with MGDA step\n",
        "            norm_sq += np.linalg.norm(grad) ** 2\n",
        "        else:\n",
        "            # Use gradient descent for blocks dependent on only one function\n",
        "            dependent_func_idx = dependent_functions[0]\n",
        "            grad_block = Jcb_f[dependent_func_idx, block_indices]\n",
        "            w[block_indices] = w[block_indices] - sz * grad_block  # Gradient descent update\n",
        "            norm_sq += np.linalg.norm(grad_block) ** 2\n",
        "\n",
        "\n",
        "    # Concatenate block gradients and calculate the norm\n",
        "    norm_list_RPMGDA.append(np.sqrt(norm_sq))\n",
        "\n",
        "\n",
        "# End measuring time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the runtime\n",
        "runtime = end_time - start_time\n",
        "print(f\"Runtime: {runtime} seconds\")\n",
        "\n",
        "\n",
        "# After the iterations, print final values\n",
        "print(\"Final w:\", w)\n",
        "y_rpmgda = f(w, D_matrix)[0]\n",
        "print(\"Final function values:\", y_rpmgda)"
      ],
      "metadata": {
        "id": "hRhqM4vXhe4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))  # Adjust figure size for better visibility\n",
        "plt.plot(norm_list_RPMGDA[:1000], linewidth=2, color='blue')  # Customize line properties\n",
        "plt.plot(norm_list[:1000], linewidth=2, color='red')  # Customize line properties\n",
        "\n",
        "plt.legend(['RPMGDA', 'MGDA'], fontsize=16)  # Add legend with font size\n",
        "plt.xlabel(\"Iteration\", fontsize=14)\n",
        "plt.ylabel(\"Gradient Norm\", fontsize=14)\n",
        "plt.title(\"Gradient Norm vs. Iteration (RP-MGDA)\", fontsize=18)\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.7)  # Add a grid for better readability\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping elements\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eu7q9JEdZl1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigate and save results"
      ],
      "metadata": {
        "id": "hWIwszqKBFuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find indices where y_rpmgda[i] >= store_func_values[i]\n",
        "indices = [i for i, (a, b) in enumerate(zip(y_rpmgda, store_func_values)) if a >= b]\n",
        "\n",
        "# Print the indices\n",
        "print(\"Indices where y_rpmgda[i] >= store_func_values[i]:\", indices)"
      ],
      "metadata": {
        "id": "NQXDjHSqfQ3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming you have store_func_values and y_rpmgda already calculated\n",
        "\n",
        "# Create a dictionary to store both variables\n",
        "data_to_store = {\n",
        "    'MGDA': store_func_values,\n",
        "    'RPMGDA': y_rpmgda\n",
        "}\n",
        "\n",
        "# Save the dictionary to a pickle file\n",
        "with open('matrix20_100_d_0p1_s3_scaling1_s100.pickle', 'wb') as file:\n",
        "    pickle.dump(data_to_store, file)"
      ],
      "metadata": {
        "id": "drUu-6Qxk2Fl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}