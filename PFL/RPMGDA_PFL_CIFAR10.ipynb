{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nthc43jmB4g"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfyqQ6W1mHxY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNL09G9XmJ3e"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_FJJFk7mMHR"
      },
      "outputs": [],
      "source": [
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOM2RG4BmQIu"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6uqWnlHH1NZ"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB-RTWKjmXR2"
      },
      "source": [
        "# Dataset loading and preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvTv-DN6mYF0"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HkvgLRYmiR_"
      },
      "outputs": [],
      "source": [
        "np.random.seed(100)\n",
        "torch.manual_seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FxHwQ1imrL8"
      },
      "outputs": [],
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLKo1OBbmwgq"
      },
      "outputs": [],
      "source": [
        "print(len(trainset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fPw5rQonJ5m"
      },
      "outputs": [],
      "source": [
        "def noniid(dataset, num_users=100, num_shards=300, num_imgs=200):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    labels = np.array(dataset.targets)\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # divide and assign shards/client\n",
        "    shard = int(num_shards/num_users)  # number of shards each user is assigned\n",
        "    for i in range(num_users):\n",
        "        rand_set = set(np.random.choice(idx_shard, shard, replace=False))\n",
        "        idx_shard = list(set(idx_shard) - rand_set)\n",
        "        for rand in rand_set:\n",
        "            dict_users[i] = np.concatenate(\n",
        "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
        "    return dict_users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9L4lcDmnKzF"
      },
      "outputs": [],
      "source": [
        "user_groups = noniid(trainset, 25, 250, 200)\n",
        "# 25 users, each user will have 2000 data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bnvAc7HsEi9"
      },
      "outputs": [],
      "source": [
        "print(len(user_groups[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGgtRaXDsJ6k"
      },
      "source": [
        "## We have 4 users, with noniid data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGvb3YaNsKjt"
      },
      "outputs": [],
      "source": [
        "np.random.seed(100)\n",
        "torch.manual_seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paKF9B8NsMoR"
      },
      "outputs": [],
      "source": [
        "user1_idxs = user_groups[0]\n",
        "user1_idxs = [int(i) for i in user1_idxs]\n",
        "user2_idxs = user_groups[1]\n",
        "user2_idxs = [int(i) for i in user2_idxs]\n",
        "user3_idxs = user_groups[2]\n",
        "user3_idxs = [int(i) for i in user3_idxs]\n",
        "user4_idxs = user_groups[3]\n",
        "user4_idxs = [int(i) for i in user4_idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh2ZRIe6shrB"
      },
      "outputs": [],
      "source": [
        "print(len(user1_idxs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyGQMmvUsQCV"
      },
      "outputs": [],
      "source": [
        "print(np.unique(np.array(trainset.targets)[user1_idxs]))\n",
        "print(np.unique(np.array(trainset.targets)[user2_idxs]))\n",
        "print(np.unique(np.array(trainset.targets)[user3_idxs]))\n",
        "print(np.unique(np.array(trainset.targets)[user4_idxs]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_aZ2uhatJkZ"
      },
      "outputs": [],
      "source": [
        "class DatasetSplit(Dataset):\n",
        "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = [int(i) for i in idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return image.clone().detach(), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFkdJWpdtKZd"
      },
      "outputs": [],
      "source": [
        "trainloader_1 = DataLoader(DatasetSplit(trainset, user1_idxs), batch_size=200, shuffle=True)\n",
        "trainloader_2 = DataLoader(DatasetSplit(trainset, user2_idxs), batch_size=200, shuffle=True)\n",
        "trainloader_3 = DataLoader(DatasetSplit(trainset, user3_idxs), batch_size=200, shuffle=True)\n",
        "trainloader_4 = DataLoader(DatasetSplit(trainset, user4_idxs), batch_size=200, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9iTGPTytMrZ"
      },
      "outputs": [],
      "source": [
        "trainloaders = [trainloader_1,trainloader_2,trainloader_3,trainloader_4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_BRxmnetR_e"
      },
      "source": [
        "# Network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzopPfAGKsou"
      },
      "source": [
        "## MLP architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqkRRgjNtSZI"
      },
      "outputs": [],
      "source": [
        "class low_MLP(nn.Module):\n",
        "    def __init__(self, dim_in, dim_hidden=256, dim_out=128):\n",
        "        super().__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout()\n",
        "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
        "        x = self.layer_input(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_hidden(x)\n",
        "        return self.relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc1fjcZqtYbV"
      },
      "outputs": [],
      "source": [
        "class top_MLP(nn.Module):\n",
        "    def __init__(self, dim_in=128, dim_hidden=128, dim_out=10):\n",
        "        super().__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout()\n",
        "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.view(-1, x.shape[0]*x.shape[1])\n",
        "        x = self.layer_input(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_hidden(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toF44wy1Kwcw"
      },
      "source": [
        "## CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWQGTNUNMe8k"
      },
      "outputs": [],
      "source": [
        "class low_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
        "        self.conv1_bn = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 5)\n",
        "        self.conv2_bn = nn.BatchNorm2d(32)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 384)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Debug input shape\n",
        "        assert x.shape[1:] == (3, 32, 32), f\"Unexpected input shape: {x.shape}\"\n",
        "\n",
        "        x = self.pool(self.relu(self.conv1_bn(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.conv2_bn(self.conv2(x))))\n",
        "\n",
        "        # Debug reshaping step\n",
        "        assert x.shape[1:] == (32, 5, 5), f\"Unexpected shape before view: {x.shape}\"\n",
        "        x = x.view(-1, 32 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEWmbEakVgXA"
      },
      "outputs": [],
      "source": [
        "class top_CNN(nn.Module):\n",
        "    # not actually a CNN, but for convenience, since this is used for CNN experiments\n",
        "    def __init__(self, dim_in=384, dim_hidden=192, dim_out=10):\n",
        "        super().__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout()\n",
        "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.view(-1, x.shape[0]*x.shape[1])\n",
        "        x = self.layer_input(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_hidden(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rsHZWbPtbNw"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pc8kloCtc-l"
      },
      "outputs": [],
      "source": [
        "img_size = 1\n",
        "for x in trainset[0][0].shape:\n",
        "    img_size *= x\n",
        "\n",
        "print(img_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX5X98HSt1eG"
      },
      "source": [
        "# MGDA functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bl8t9Mpt11M"
      },
      "outputs": [],
      "source": [
        "!pip install quadprog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6kUa6Mb4kxG"
      },
      "outputs": [],
      "source": [
        "import quadprog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuBXRPp140GZ"
      },
      "outputs": [],
      "source": [
        "def solve_w(U):\n",
        "    # U is list of gradients (stored as list of tensors) from n users\n",
        "    # That's why the following code might seem a bit clumsy (e.g. not using 2d matrix operation directly)\n",
        "\n",
        "    n = len(U)\n",
        "    K = np.eye(n,dtype=float)\n",
        "    for i in range(0,n):\n",
        "        for j in range(0,n):\n",
        "            K[i,j] = 0\n",
        "            for t in range(len(U[i])):\n",
        "                K[i,j] += torch.mul(U[i][t],U[j][t]).sum()\n",
        "\n",
        "    Q = 0.5 *(K + K.T)\n",
        "    p = np.zeros(n,dtype=float)\n",
        "    a = np.ones(n,dtype=float).reshape(-1,1)\n",
        "    Id = np.eye(n,dtype=float)\n",
        "    A = np.concatenate((a,Id),axis=1)\n",
        "    b = np.zeros(n+1)\n",
        "    b[0] = 1.\n",
        "    # grad = np.zeros(d,dtype=float) # d is not defined\n",
        "    # # grad = np.zeros(n,dtype=float)\n",
        "    try:\n",
        "        alpha = quadprog.solve_qp(Q,p,A,b)[0]\n",
        "    except ValueError as v:\n",
        "        print('MGDA stops since the min norm element is zero')\n",
        "    return alpha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqvZVgmQ48SX"
      },
      "source": [
        "# RP-MGDA on PFL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting"
      ],
      "metadata": {
        "id": "pZTT34amWNpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAVW7rys48pt"
      },
      "outputs": [],
      "source": [
        "epochs = 50 # Change to 50 for warm start\n",
        "lr = 0.005\n",
        "loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train (RP-MGDA)"
      ],
      "metadata": {
        "id": "g5DEtjiIWQsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbbgeD027WI1"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(100) # 42,100\n",
        "\n",
        "top_net = top_CNN().to(device)\n",
        "model1 = low_CNN().to(device)\n",
        "model2 = low_CNN().to(device)\n",
        "model3 = low_CNN().to(device)\n",
        "model4 = low_CNN().to(device)\n",
        "\n",
        "models = [model1,model2,model3,model4]\n",
        "\n",
        "top_net.train()\n",
        "model1.train()\n",
        "model2.train()\n",
        "model3.train()\n",
        "model4.train()\n",
        "loss_list = []\n",
        "\n",
        "lr=lr\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    if epoch % 200 == 199:\n",
        "        lr *= 0.9\n",
        "    # Back up the previous top_net parameters\n",
        "    old_weights = copy.deepcopy(top_net.state_dict())\n",
        "\n",
        "    # Re-init grads and losses\n",
        "    grads = []\n",
        "    losses = []\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        for batch_idx, (X, y) in enumerate(trainloaders[i]):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            low_model = models[i]\n",
        "            h = low_model(X)\n",
        "            output = top_net(h)\n",
        "            l = loss(output,y)\n",
        "\n",
        "            l.backward()\n",
        "            with torch.no_grad():\n",
        "                # Store top_net gradient for later MGDA\n",
        "                gradient = []\n",
        "                for p in top_net.parameters():\n",
        "                    gradient.append(p.grad.clone())\n",
        "                grads.append(gradient)\n",
        "\n",
        "                # Update lower model\n",
        "                for p in low_model.parameters():\n",
        "                    p -= p.grad * lr\n",
        "                low_model.zero_grad()\n",
        "                top_net.zero_grad()\n",
        "\n",
        "                losses.append(l.item())\n",
        "\n",
        "    gradient_coefficients = solve_w(grads)\n",
        "    # print(\"gradient coefficients for epoch \",epoch, \"is \", gradient_coefficients)\n",
        "\n",
        "    # Update top_net model\n",
        "    with torch.no_grad():\n",
        "        for i, key in enumerate(old_weights.keys()):\n",
        "            for j in range(len(gradient_coefficients)):\n",
        "                old_weights[key] -= lr * gradient_coefficients[j] * grads[j][i]\n",
        "\n",
        "    top_net.load_state_dict(old_weights)\n",
        "\n",
        "    print(losses)\n",
        "\n",
        "    loss_list.append(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TbztZS77dlK"
      },
      "outputs": [],
      "source": [
        "print(loss_list[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Vk5c017eYL"
      },
      "outputs": [],
      "source": [
        "top_net.zero_grad()\n",
        "model1.zero_grad()\n",
        "model2.zero_grad()\n",
        "model3.zero_grad()\n",
        "model4.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVCdfiiE7iHv"
      },
      "source": [
        "## Save RPMGDA results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhP2_siD7Xuo"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qzyHawS7lgg"
      },
      "outputs": [],
      "source": [
        "with open('CIFAR10_seed100_RPMGDA_EP5000_lr0p005.pickle','wb') as f:\n",
        "    pickle.dump(loss_list,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKbWPehobYHl"
      },
      "source": [
        "# MGDA on PFL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MGDA implementation"
      ],
      "metadata": {
        "id": "LpUJkHAuVl-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igyMmWMAbYcn"
      },
      "outputs": [],
      "source": [
        "def augment_gradient(top_grad, low_grad, idx):\n",
        "    \"\"\"\n",
        "    top_grad = [topnet_layer1_w,topnet_layer1_b,topnet_layer2_w,topnet_layer2_b,...] etc.\n",
        "    low_grad = [lownet_layer1_w,lownet_layer1_b,lownet_layer2_w,lownet_layer2_b,...] etc.\n",
        "    idx = index of the user (0,1,2,3), used in trainloaders[idx]\n",
        "\n",
        "    Output: augmented gradient, used for later on MGDA aggregation\n",
        "    \"\"\"\n",
        "    pad_zeros_grad = [0] * len(low_grad)\n",
        "    for i in range(len(low_grad)):\n",
        "        pad_zeros_grad[i] = torch.zeros_like(low_grad[i])\n",
        "\n",
        "    if idx == 0:\n",
        "        augmented_grad = top_grad + low_grad + pad_zeros_grad + pad_zeros_grad + pad_zeros_grad\n",
        "    elif idx == 1:\n",
        "        augmented_grad = top_grad + pad_zeros_grad + low_grad + pad_zeros_grad + pad_zeros_grad\n",
        "    elif idx == 2:\n",
        "        augmented_grad = top_grad + pad_zeros_grad + pad_zeros_grad + low_grad + pad_zeros_grad\n",
        "    elif idx == 3:\n",
        "        augmented_grad = top_grad + pad_zeros_grad + pad_zeros_grad + pad_zeros_grad + low_grad\n",
        "    else:\n",
        "        print(\"Error in augmenting gradient, index is not valid: \", idx)\n",
        "\n",
        "    return augmented_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYsN1Ifqbcy2"
      },
      "outputs": [],
      "source": [
        "def solve_padded_w(U):\n",
        "    # U is list of gradients (stored as list of tensors) from n users\n",
        "\n",
        "    n = len(U)\n",
        "    K = np.eye(n,dtype=float)\n",
        "    for i in range(0,n):\n",
        "        for j in range(0,n):\n",
        "            K[i,j] = 0\n",
        "            for t in range(len(U[i])):\n",
        "                K[i,j] += torch.mul(U[i][t],U[j][t]).sum()\n",
        "\n",
        "    Q = 0.5 *(K + K.T)\n",
        "    p = np.zeros(n,dtype=float)\n",
        "    a = np.ones(n,dtype=float).reshape(-1,1)\n",
        "    Id = np.eye(n,dtype=float)\n",
        "    A = np.concatenate((a,Id),axis=1)\n",
        "    b = np.zeros(n+1)\n",
        "    b[0] = 1.\n",
        "    # grad = np.zeros(d,dtype=float)\n",
        "    # # grad = np.zeros(n,dtype=float)\n",
        "    try:\n",
        "        alpha = quadprog.solve_qp(Q,p,A,b)[0]\n",
        "    except ValueError as v:\n",
        "        print('MGDA stops since the min norm element is zero')\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhmyHKFZbfFz"
      },
      "source": [
        "## Setting\n",
        "Instruction: \\\n",
        "(1) Go to the previous RP-MGDA setting cell, change epochs=50 \\\n",
        "(2) Run Train (RP-MGDA), which warm starts the network weights \\\n",
        "(3) Now we can run the following for MGDA training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZU1YCQIbfgY"
      },
      "outputs": [],
      "source": [
        "epochs = 5000\n",
        "lr = 0.005\n",
        "loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train (MGDA)"
      ],
      "metadata": {
        "id": "lWEMQoKWWf_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc6vvoXcblYE"
      },
      "outputs": [],
      "source": [
        "loss_list = []\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    if epoch % 200 == 149:\n",
        "        lr *= 0.9\n",
        "    # Back up the previous top_net parameters\n",
        "    old_topnet_weights = copy.deepcopy(top_net.state_dict())\n",
        "\n",
        "    # Re-init grads and losses\n",
        "    grads = []\n",
        "    losses = []\n",
        "\n",
        "    for i in range(len(models)):\n",
        "        for batch_idx, (X, y) in enumerate(trainloaders[i]):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            low_model = models[i]\n",
        "            h = low_model(X)\n",
        "            output = top_net(h)\n",
        "            l = loss(output,y)\n",
        "\n",
        "            l.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Store top_net gradient for later MGDA\n",
        "                topnet_gradient = []\n",
        "                for p in top_net.parameters():\n",
        "                    topnet_gradient.append(p.grad.clone())\n",
        "\n",
        "                lowernet_gradient = []\n",
        "                for p in low_model.parameters():\n",
        "                    lowernet_gradient.append(p.grad.clone())\n",
        "\n",
        "                augmented_gradient = augment_gradient(topnet_gradient, lowernet_gradient, i)\n",
        "\n",
        "                grads.append(augmented_gradient)\n",
        "\n",
        "                losses.append(l.item())\n",
        "\n",
        "\n",
        "    # MGDA\n",
        "    gradient_coefficients = solve_padded_w(grads)\n",
        "    # print(\"gradient coefficients for epoch \",epoch, \"is \", gradient_coefficients)\n",
        "\n",
        "    # Update lower models\n",
        "    for i in range(len(models)):\n",
        "        with torch.no_grad():\n",
        "            for p in models[i].parameters():\n",
        "                p -= p.grad * lr * gradient_coefficients[i]\n",
        "            models[i].zero_grad()\n",
        "            top_net.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "    # Update top_net model\n",
        "    with torch.no_grad():\n",
        "        for i, key in enumerate(old_topnet_weights.keys()):\n",
        "            for j in range(len(gradient_coefficients)):\n",
        "                old_topnet_weights[key] -= lr * gradient_coefficients[j] * grads[j][i]\n",
        "\n",
        "    top_net.load_state_dict(old_topnet_weights)\n",
        "\n",
        "\n",
        "\n",
        "    print(losses)\n",
        "\n",
        "    loss_list.append(losses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save MGDA results"
      ],
      "metadata": {
        "id": "0PyqNl5bXm60"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUd4vRGNbvnp"
      },
      "outputs": [],
      "source": [
        "with open('CIFAR10_seed100_MGDA_EP5000_lr0p005.pickle','wb') as f:\n",
        "    pickle.dump(loss_list,f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}