{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5NYWG6ZL7wz"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "lA1MQrCKDcR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "-B83DtFzDdCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import copy"
      ],
      "metadata": {
        "id": "CYonMXEXDgec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset loading and preprocessing"
      ],
      "metadata": {
        "id": "mE4aNiSrKy6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "FHGSsCkdKuKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_categories = ['non_living', 'living']\n",
        "\n",
        "mid_categories = ['ground_vehicle', 'non_ground_vehicle', 'land_animals', 'non_land_animals']\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "vIwQLI9kXRkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetSplit(Dataset):\n",
        "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = [int(i) for i in idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return image.clone().detach(), label"
      ],
      "metadata": {
        "id": "oVZaLGMKkC39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(100)\n",
        "torch.manual_seed(100)"
      ],
      "metadata": {
        "id": "cmskgK6yYicp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the labels of the original training set\n",
        "original_trainset_labels = np.array(trainset.targets)\n",
        "\n",
        "# Calculate the number of samples per class in the subset\n",
        "samples_per_class = len(trainset) // 25 // len(classes)\n",
        "\n",
        "# Initialize an empty list to store the subset indices\n",
        "subset_indices = []\n",
        "\n",
        "# Iterate over each class and sample indices\n",
        "for class_idx in range(len(classes)):\n",
        "    # Get the indices of samples belonging to the current class\n",
        "    class_indices = np.where(original_trainset_labels == class_idx)[0]\n",
        "\n",
        "    # Randomly sample indices from the current class\n",
        "    class_subset_indices = np.random.choice(class_indices, size=samples_per_class, replace=False)\n",
        "\n",
        "    # Append the sampled indices to the subset indices list\n",
        "    subset_indices.extend(class_subset_indices)\n",
        "\n",
        "# Create a subset dataset using the sampled indices\n",
        "subset_trainset = DatasetSplit(trainset, subset_indices)\n",
        "\n",
        "# Create a DataLoader for the subset\n",
        "subset_trainloader = DataLoader(subset_trainset, batch_size=2000, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "aIdK4j0vWvVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "mYQaIORyK8Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def class_to_top_cat(class_label):\n",
        "    # input is a index between 0 to 9\n",
        "    class_label = int(class_label)\n",
        "    if class_label in [0,1,8,9]:\n",
        "        return 0\n",
        "    elif class_label in [2,3,4,5,6,7]:\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"Class label is not valid\")\n",
        "        return -1\n",
        "\n",
        "def class_to_mid_cat(class_label):\n",
        "    # input is a index between 0 to 9\n",
        "    class_label = int(class_label)\n",
        "    if class_label in [1,9]:\n",
        "        return 0\n",
        "    elif class_label in [0,8]:\n",
        "        return 1\n",
        "    elif class_label in [3,4,5,7]:\n",
        "        return 2\n",
        "    elif class_label in [2,6]:\n",
        "        return 3\n",
        "    else:\n",
        "        print(\"Class label is not valid\")\n",
        "        return -1"
      ],
      "metadata": {
        "id": "eu6LTxolNrm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "for i in range(0,10):\n",
        "    print(\"a \", classes[i], \" is a \", top_categories[class_to_top_cat(i)], mid_categories[class_to_mid_cat(i)])"
      ],
      "metadata": {
        "id": "Qk2ifJSLO6rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MGDA functions"
      ],
      "metadata": {
        "id": "gC6SbcgsY0AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quadprog"
      ],
      "metadata": {
        "id": "Ccz2Yls7L3VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import quadprog"
      ],
      "metadata": {
        "id": "DlQLPTbrY3Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_w(U):\n",
        "    # U is list of gradients (stored as list of tensors) from n users\n",
        "    # That's why the following code might seem a bit clumsy (e.g. not using 2d matrix operation directly)\n",
        "\n",
        "    n = len(U)\n",
        "    K = np.eye(n,dtype=float)\n",
        "    for i in range(0,n):\n",
        "        for j in range(0,n):\n",
        "            K[i,j] = 0\n",
        "            for t in range(len(U[i])):\n",
        "                K[i,j] += torch.mul(U[i][t],U[j][t]).sum()\n",
        "\n",
        "    Q = 0.5 *(K + K.T)\n",
        "    p = np.zeros(n,dtype=float)\n",
        "    a = np.ones(n,dtype=float).reshape(-1,1)\n",
        "    Id = np.eye(n,dtype=float)\n",
        "    A = np.concatenate((a,Id),axis=1)\n",
        "    b = np.zeros(n+1)\n",
        "    b[0] = 1.\n",
        "    # grad = np.zeros(d,dtype=float) # d is not defined\n",
        "    # # grad = np.zeros(n,dtype=float)\n",
        "    try:\n",
        "        alpha = quadprog.solve_qp(Q,p,A,b)[0]\n",
        "    except ValueError as v:\n",
        "        print('MGDA stops since the min norm element is zero')\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "uGkT3RN9Y4s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_padded_w(U):\n",
        "    # U is list of gradients (stored as list of tensors) from n users\n",
        "\n",
        "    n = len(U)\n",
        "    K = np.eye(n,dtype=float)\n",
        "    for i in range(0,n):\n",
        "        for j in range(0,n):\n",
        "            K[i,j] = 0\n",
        "            for t in range(len(U[i])):\n",
        "                K[i,j] += torch.mul(U[i][t],U[j][t]).sum()\n",
        "\n",
        "    Q = 0.5 *(K + K.T)\n",
        "    p = np.zeros(n,dtype=float)\n",
        "    a = np.ones(n,dtype=float).reshape(-1,1)\n",
        "    Id = np.eye(n,dtype=float)\n",
        "    A = np.concatenate((a,Id),axis=1)\n",
        "    b = np.zeros(n+1)\n",
        "    b[0] = 1.\n",
        "    # grad = np.zeros(d,dtype=float)\n",
        "    # # grad = np.zeros(n,dtype=float)\n",
        "    try:\n",
        "        alpha = quadprog.solve_qp(Q,p,A,b)[0]\n",
        "    except ValueError as v:\n",
        "        print('MGDA stops since the min norm element is zero')\n",
        "    return alpha\n"
      ],
      "metadata": {
        "id": "fSB5Snk45Xb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "bvsgWLJZZpHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Architecture"
      ],
      "metadata": {
        "id": "1CRSeS-sZrzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'FEX' means 'Feature EXtract'\n",
        "\n",
        "class first_FEX_layer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
        "        self.conv1_bn = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 5)\n",
        "        self.conv2_bn = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1_bn(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.conv2_bn(self.conv2(x))))\n",
        "        return x\n",
        "\n",
        "class second_FEX_layer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 384)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "class third_FEX_layer(nn.Module):\n",
        "    def __init__(self, dim_in=384, dim_hidden=192):\n",
        "        super().__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout()\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer_input(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class first_classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layerout = nn.Linear(32 * 5 * 5, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 5 * 5)\n",
        "        x = self.layerout(x)\n",
        "        return x\n",
        "\n",
        "class second_classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layerout = nn.Linear(384, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layerout(x)\n",
        "        return x\n",
        "\n",
        "class third_classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layerout = nn.Linear(192, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layerout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "t4JHx6RzY7s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "0G71mQkLjyeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training （RP-MGDA）"
      ],
      "metadata": {
        "id": "48t1YZwSkSiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2005 # Change to 5 for warm start\n",
        "lr = 0.008\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "z84QQAb8b_mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100) # 42,100\n",
        "\n",
        "# 'ftet' means feature extract\n",
        "\n",
        "ftxt_layer1 = first_FEX_layer().to(device)\n",
        "ftxt_layer2 = second_FEX_layer().to(device)\n",
        "ftxt_layer3 = third_FEX_layer().to(device)\n",
        "classifier1 = first_classifier().to(device)\n",
        "classifier2 = second_classifier().to(device)\n",
        "classifier3 = third_classifier().to(device)\n",
        "\n",
        "ftxt_layer1.train()\n",
        "ftxt_layer2.train()\n",
        "ftxt_layer3.train()\n",
        "classifier1.train()\n",
        "classifier2.train()\n",
        "classifier3.train()\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "lr=lr\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    if epoch % 200 == 199:\n",
        "        lr *= 0.8\n",
        "\n",
        "    # Back up the previous shared layers' parameters\n",
        "    ft1_old_weights = copy.deepcopy(ftxt_layer1.state_dict())\n",
        "    ft2_old_weights = copy.deepcopy(ftxt_layer2.state_dict())\n",
        "\n",
        "    # Re-init grads and losses\n",
        "    grads = []\n",
        "    losses = []\n",
        "\n",
        "    # Take a batch\n",
        "    for batch_idx, (X, y) in enumerate(subset_trainloader):\n",
        "        with torch.no_grad():\n",
        "            top_label = y.apply_(class_to_top_cat).to(device)\n",
        "            mid_label = y.apply_(class_to_mid_cat).to(device)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        h1 = ftxt_layer1(X)\n",
        "        h2 = ftxt_layer2(h1)\n",
        "        h3 = ftxt_layer3(h2)\n",
        "        output1 = classifier1(h1)\n",
        "        output2 = classifier2(h2)\n",
        "        output3 = classifier3(h3)\n",
        "        l1 = loss(output1,top_label)\n",
        "        l2 = loss(output2,mid_label)\n",
        "        l3 = loss(output3,y)\n",
        "\n",
        "        # 1st pass, top categories\n",
        "        l1.backward(retain_graph=True)\n",
        "        with torch.no_grad():\n",
        "            # Store ftxt_layer1 gradient for later MGDA\n",
        "            ft1_gradient_l1 = []\n",
        "            for p in ftxt_layer1.parameters():\n",
        "                ft1_gradient_l1.append(p.grad.clone())\n",
        "\n",
        "            # Update classifier1\n",
        "            for p in classifier1.parameters():\n",
        "                p -= p.grad * lr\n",
        "            classifier1.zero_grad()\n",
        "            ftxt_layer1.zero_grad()\n",
        "\n",
        "            losses.append(l1.item())\n",
        "\n",
        "        # 2nd pass, mid categories\n",
        "        l2.backward(retain_graph=True)\n",
        "        with torch.no_grad():\n",
        "            # Store ftxt_layer1 and ftxt_later2 gradient for later MGDA\n",
        "            ft1_gradient_l2 = []\n",
        "            for p in ftxt_layer1.parameters():\n",
        "                ft1_gradient_l2.append(p.grad.clone())\n",
        "\n",
        "            ft2_gradient_l2 = []\n",
        "            for p in ftxt_layer2.parameters():\n",
        "                ft2_gradient_l2.append(p.grad.clone())\n",
        "\n",
        "            # Update classifier2\n",
        "            for p in classifier2.parameters():\n",
        "                p -= p.grad * lr\n",
        "            classifier2.zero_grad()\n",
        "            ftxt_layer1.zero_grad()\n",
        "            ftxt_layer2.zero_grad()\n",
        "\n",
        "            losses.append(l2.item())\n",
        "\n",
        "        # 3rd pass, 10 labels\n",
        "        l3.backward(retain_graph=False)\n",
        "        with torch.no_grad():\n",
        "            # Store ftxt_layer1, ftxt_layer2 gradient for later MGDA\n",
        "            # ftxt_layer3 (in RP-MGDA only) and classifier3 can be updated immediately\n",
        "            ft1_gradient_l3 = []\n",
        "            for p in ftxt_layer1.parameters():\n",
        "                ft1_gradient_l3.append(p.grad.clone())\n",
        "\n",
        "            ft2_gradient_l3 = []\n",
        "            for p in ftxt_layer2.parameters():\n",
        "                ft2_gradient_l3.append(p.grad.clone())\n",
        "\n",
        "            # Update ftxt_layer3\n",
        "            for p in ftxt_layer3.parameters():\n",
        "                p -= p.grad * lr\n",
        "\n",
        "            # Update classifier3\n",
        "            for p in classifier3.parameters():\n",
        "                p -= p.grad * lr\n",
        "            ftxt_layer1.zero_grad()\n",
        "            ftxt_layer2.zero_grad()\n",
        "            ftxt_layer3.zero_grad()\n",
        "            classifier3.zero_grad()\n",
        "\n",
        "            losses.append(l3.item())\n",
        "\n",
        "        # MGDA on ft1_gradient + ft2_gradient\n",
        "        # first augment gradient_l1 (ft1_gradient_l1 and zeros_like ft2_gradient (e.g. ft2_gradient_l2))\n",
        "        zeros_like_ft2 = [torch.zeros_like(p) for p in ft2_gradient_l2]\n",
        "        grad1 = ft1_gradient_l1 + zeros_like_ft2\n",
        "        grad2 = ft1_gradient_l2 + ft2_gradient_l2\n",
        "        grad3 = ft1_gradient_l3 + ft2_gradient_l3\n",
        "        grads = [grad1, grad2, grad3]\n",
        "        ft1_grads = [ft1_gradient_l1, ft1_gradient_l2, ft1_gradient_l3]\n",
        "        ft2_grads = [zeros_like_ft2, ft2_gradient_l2, ft2_gradient_l3]\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gradient_coefficients = solve_padded_w(grads)\n",
        "            print(\"gradient coefficients for epoch \",epoch, \"is \", gradient_coefficients)\n",
        "\n",
        "\n",
        "        # Update ftxt_layer1\n",
        "        with torch.no_grad():\n",
        "            for i, (name, param) in enumerate(ftxt_layer1.named_parameters()):\n",
        "                for j in range(len(gradient_coefficients)):\n",
        "                    ft1_old_weights[name] -= lr * gradient_coefficients[j] * ft1_grads[j][i]\n",
        "        ftxt_layer1.load_state_dict(ft1_old_weights)\n",
        "\n",
        "        # Update ftxt_layer2\n",
        "        with torch.no_grad():\n",
        "            for i, (name, param) in enumerate(ftxt_layer2.named_parameters()):\n",
        "                for j in range(len(gradient_coefficients)):\n",
        "                    ft2_old_weights[name] -= lr * gradient_coefficients[j] * ft2_grads[j][i]\n",
        "        ftxt_layer2.load_state_dict(ft2_old_weights)\n",
        "\n",
        "    print(losses)\n",
        "\n",
        "    loss_list.append(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ePNuiyxMoXJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(losses)"
      ],
      "metadata": {
        "id": "dXV0yKMjw-IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ftxt_layer1.zero_grad()\n",
        "ftxt_layer2.zero_grad()\n",
        "ftxt_layer3.zero_grad()\n",
        "classifier1.zero_grad()\n",
        "classifier2.zero_grad()\n",
        "classifier3.zero_grad()"
      ],
      "metadata": {
        "id": "zp3-l7enOj6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "u4hF-mTdO0ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('seed100_RPMGDA_LADDER_losses_EP2000_lr0p008_warmstart.pickle','wb') as f:\n",
        "    pickle.dump(loss_list,f)"
      ],
      "metadata": {
        "id": "dBcSzIVg7ii0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training (MGDA)"
      ],
      "metadata": {
        "id": "j7xKBOJcT3l4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction:\n",
        "(1) Go to Training (RP-MGDA), set epochs=5 for warmstart, and run\\\n",
        "(2) After that, run the following for MGDA training"
      ],
      "metadata": {
        "id": "5db7iWuDcq0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2000\n",
        "lr = 0.008\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "sHALfZgeUDLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "lr=lr\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    if epoch % 200 == 194:\n",
        "        lr *= 0.8\n",
        "\n",
        "    # Back up the previous shared layers' parameters\n",
        "    ft1_old_weights = copy.deepcopy(ftxt_layer1.state_dict())\n",
        "    ft2_old_weights = copy.deepcopy(ftxt_layer2.state_dict())\n",
        "    ft3_old_weights = copy.deepcopy(ftxt_layer3.state_dict())\n",
        "\n",
        "    # Re-init grads and losses\n",
        "    grads = []\n",
        "    losses = []\n",
        "\n",
        "    # Take a batch\n",
        "    for batch_idx, (X, y) in enumerate(subset_trainloader):\n",
        "        with torch.no_grad():\n",
        "            top_label = y.apply_(class_to_top_cat).to(device)\n",
        "            mid_label = y.apply_(class_to_mid_cat).to(device)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        h1 = ftxt_layer1(X)\n",
        "        h2 = ftxt_layer2(h1)\n",
        "        h3 = ftxt_layer3(h2)\n",
        "        output1 = classifier1(h1)\n",
        "        output2 = classifier2(h2)\n",
        "        output3 = classifier3(h3)\n",
        "        l1 = loss(output1,top_label)\n",
        "        l2 = loss(output2,mid_label)\n",
        "        l3 = loss(output3,y)\n",
        "\n",
        "        # 1st pass, top categories\n",
        "\n",
        "        l1.backward(retain_graph=True)\n",
        "        with torch.no_grad():\n",
        "            # Store ftxt_layer1 gradient for later MGDA\n",
        "            ft1_gradient_l1 = []\n",
        "            for p in ftxt_layer1.parameters():\n",
        "                ft1_gradient_l1.append(p.grad.clone())\n",
        "\n",
        "            # Update classifier1\n",
        "            for p in classifier1.parameters():\n",
        "                p -= p.grad * lr\n",
        "            classifier1.zero_grad()\n",
        "            ftxt_layer1.zero_grad()\n",
        "\n",
        "            losses.append(l1.item())\n",
        "\n",
        "\n",
        "\n",
        "        # 2nd pass, mid categories\n",
        "        l2.backward(retain_graph=True)\n",
        "        with torch.no_grad():\n",
        "            # Store ftxt_layer1 and ftxt_later2 gradient for later MGDA\n",
        "            ft1_gradient_l2 = []\n",
        "            for p in ftxt_layer1.parameters():\n",
        "                ft1_gradient_l2.append(p.grad.clone())\n",
        "\n",
        "            ft2_gradient_l2 = []\n",
        "            for p in ftxt_layer2.parameters():\n",
        "                ft2_gradient_l2.append(p.grad.clone())\n",
        "\n",
        "            # Update classifier2\n",
        "            for p in classifier2.parameters():\n",
        "                p -= p.grad * lr\n",
        "            classifier2.zero_grad()\n",
        "            ftxt_layer1.zero_grad()\n",
        "            ftxt_layer2.zero_grad()\n",
        "\n",
        "            losses.append(l2.item())\n",
        "\n",
        "        # 3rd pass, label\n",
        "        l3.backward(retain_graph=False)\n",
        "        with torch.no_grad():\n",
        "            # Store ftxt_layer1, ftxt_layer2, ftxt_layer3 gradient for later MGDA\n",
        "            # Only classifier3 can be updated immediately (this is different compared to RPMGDA)\n",
        "            ft1_gradient_l3 = []\n",
        "            for p in ftxt_layer1.parameters():\n",
        "                ft1_gradient_l3.append(p.grad.clone())\n",
        "\n",
        "            ft2_gradient_l3 = []\n",
        "            for p in ftxt_layer2.parameters():\n",
        "                ft2_gradient_l3.append(p.grad.clone())\n",
        "\n",
        "            ft3_gradient_l3 = []\n",
        "            for p in ftxt_layer3.parameters():\n",
        "                ft3_gradient_l3.append(p.grad.clone())\n",
        "\n",
        "            # Update classifier3\n",
        "            for p in classifier3.parameters():\n",
        "                p -= p.grad * lr\n",
        "\n",
        "            ftxt_layer1.zero_grad()\n",
        "            ftxt_layer2.zero_grad()\n",
        "            ftxt_layer3.zero_grad()\n",
        "            classifier3.zero_grad()\n",
        "\n",
        "            losses.append(l3.item())\n",
        "\n",
        "        # MGDA on ft1_gradient + ft2_gradient + ft3_gradient\n",
        "        # first augment gradient_l1 (ft1_gradient_l1 and zeros_like ft2_gradient (e.g. ft2_gradient_l2) and more)\n",
        "        # also augment gradient_l2\n",
        "\n",
        "        zeros_like_ft2 = [torch.zeros_like(p) for p in ft2_gradient_l2]\n",
        "        zeros_like_ft3 = [torch.zeros_like(p) for p in ft3_gradient_l3]\n",
        "        grad1 = ft1_gradient_l1 + zeros_like_ft2 + zeros_like_ft3\n",
        "        grad2 = ft1_gradient_l2 + ft2_gradient_l2 + zeros_like_ft3\n",
        "        grad3 = ft1_gradient_l3 + ft2_gradient_l3 + ft3_gradient_l3\n",
        "        grads = [grad1, grad2, grad3]\n",
        "        ft1_grads = [ft1_gradient_l1, ft1_gradient_l2, ft1_gradient_l3]\n",
        "        ft2_grads = [zeros_like_ft2, ft2_gradient_l2, ft2_gradient_l3]\n",
        "        ft3_grads = [zeros_like_ft3, zeros_like_ft3, ft3_gradient_l3]\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gradient_coefficients = solve_padded_w(grads)\n",
        "            print(\"gradient coefficients for epoch \",epoch, \"is \", gradient_coefficients)\n",
        "\n",
        "\n",
        "        # Update ftxt_layer1\n",
        "        with torch.no_grad():\n",
        "            for i, (name, param) in enumerate(ftxt_layer1.named_parameters()):\n",
        "                for j in range(len(gradient_coefficients)):\n",
        "                    ft1_old_weights[name] -= lr * gradient_coefficients[j] * ft1_grads[j][i]\n",
        "        ftxt_layer1.load_state_dict(ft1_old_weights)\n",
        "\n",
        "        # Update ftxt_layer2\n",
        "        with torch.no_grad():\n",
        "            for i, (name, param) in enumerate(ftxt_layer2.named_parameters()):\n",
        "                for j in range(len(gradient_coefficients)):\n",
        "                    ft2_old_weights[name] -= lr * gradient_coefficients[j] * ft2_grads[j][i]\n",
        "        ftxt_layer2.load_state_dict(ft2_old_weights)\n",
        "\n",
        "        # Update ftxt_layer3\n",
        "        with torch.no_grad():\n",
        "            for i, (name, param) in enumerate(ftxt_layer3.named_parameters()):\n",
        "                for j in range(len(gradient_coefficients)):\n",
        "                    ft3_old_weights[name] -= lr * gradient_coefficients[j] * ft3_grads[j][i]\n",
        "        ftxt_layer3.load_state_dict(ft3_old_weights)\n",
        "\n",
        "    print(losses)\n",
        "\n",
        "    loss_list.append(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ5I5CsXWmhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('seed100_MGDA_LADDER_losses_EP2000_lr0p008_warmstart.pickle','wb') as f:\n",
        "    pickle.dump(loss_list,f)"
      ],
      "metadata": {
        "id": "9y5dIfDer1rQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}